#Install and load required packages
```{r}
#install.packages("tm")
#install.packages("text2vec")
#install.packages("SnowballC")
library(tm)
library(text2vec)
library(SnowballC)
```

### Reading the Transcripts
```{r}
data <- read.csv("ted_transcripts.csv", header = F, sep = "|")
doc <- 0
for (i in c(2:100)) {doc[i] <- as.character(data$V1[i])}
doc.list <- as.list(doc[2:100])
N.docs <- length(doc.list)
names(doc.list) <- paste0("Doc", c(1:N.docs))
Query <- as.character(data$V1[1])
```

## Preparing the Corpus
```{r}
my.docs <- VectorSource(c(doc.list, Query))
my.docs$Names <- c(names(doc.list), "Query")
my.corpus <- Corpus(my.docs)
my.corpus
```


### Cleaning and Preprocessing the text
```{r}
clean.corpus <- tm_map(my.corpus, removePunctuation, ucp = TRUE)
clean.corpus <- tm_map(clean.corpus, removeWords, stopwords(kind = "en"))
clean.corpus <- tm_map(clean.corpus, stemDocument)

# removePuncutation was used to remove all punctuation. This allows for the amount of bags needed to classify words be minimalized, 
# allowing all similar words to be classified togethered (ex. Hello! and hello, will be different unless punctuation is removed). 
# This minimizes the amount of spaces that need to be searched. Making the document comparison more efficent.Ucp was changed to true
# in order for the Unciode general category to be used  instead of ASCII which is a subset of Unicode, this allows for a larger variety 
# of punction to be removed.

# removeWords and stopwords, combine to remove all of the commonly used words in the documents (ex. the, and, is) this allows the 
# queries to focuses the search on important words. Allowing for a more efficent search.The third technique used was the stemDocument
# function, this is necessary for cleaning and processing text as it reduces words to their stem by removing prefixes and suffixes.
# Therefore allowing more comprehensive and faster results to queries (ex. ability turns into abil). 
```

### Creating Term Document Matrix
```{r}
term.doc.matrix <- TermDocumentMatrix(clean.corpus)
inspect(term.doc.matrix[1:10,1:10])
```

### Converting the TDM into a matrix and displaying the head and dimensions.
```{r}
term.doc.matrix <- as.matrix(term.doc.matrix)
head(term.doc.matrix)
dim(term.doc.matrix)
```

## Declaring weights (TF-IDF)
```{r}
get.tf.idf.weights <- function(tf.vec) {
  n.docs <- length(tf.vec)
  doc.frequency <- length(tf.vec[tf.vec > 0])
  weights <- rep(0, length(tf.vec))
  relative.frequency <- tf.vec[tf.vec > 0] / sum(tf.vec[tf.vec > 0])
  weights[tf.vec > 0] <-  relative.frequency * log(n.docs/doc.frequency)
  return(weights)
}
# Computes the tfidf weights from the term frequency vector
```

### Declaring weights (TF-IDF variants)
# There are three TF-IDF variants that can be used for different word weighting and document sizes.
```{r}

# Variant 1
V1 <- function(tf.vec) {
  # Computes the tfidf weights from the term frequency vector
  n.docs <- length(tf.vec)
  doc.frequency <- length(tf.vec[tf.vec > 0])
  weights <- rep(0, length(tf.vec))
  relative.frequency <- tf.vec[tf.vec > 0] / sum(tf.vec[tf.vec > 0])
  weights[tf.vec > 0] <-  relative.frequency * log(n.docs/doc.frequency)
  return(weights)
}
# Variant 2
get.tf.idf.weights2 <- function(tf.vec) {
  n.docs <- length(tf.vec)
  doc.frequency <- length(tf.vec[tf.vec > 0])
  weights <- rep(0, length(tf.vec))
  relative.frequency <- 1 +(log(tf.vec[tf.vec > 0]))
  weights[tf.vec > 0] <-  relative.frequency * log(1+(n.docs/doc.frequency))
  return(weights)
}
# Variant 3
get.tf.idf.weights3 <- function(tf.vec) {
  n.docs <- length(tf.vec)
  doc.frequency <- length(tf.vec[tf.vec > 0])
  weights <- rep(0, length(tf.vec))
  relative.frequency <- tf.vec[tf.vec > 0]
  weights[tf.vec > 0] <- relative.frequency * log(n.docs/doc.frequency)
  return(weights)
}
```

### Compute Cosine Similarity and Display heatmap
# Substitue variants 
```{r}
tfidf.matrix <- t(apply(term.doc.matrix, 1,
                        FUN = function(row) {get.tf.idf.weights(row)}))
colnames(tfidf.matrix) <- my.docs$Names

head(tfidf.matrix)
dim(tfidf.matrix)


similarity.matrix <- sim2(t(tfidf.matrix), method = 'cosine')
heatmap(similarity.matrix)
```

### View Results
```{r}
sort(similarity.matrix["Query", ], decreasing = TRUE)[1:10]
```

### Comparative analyses
```{r}
# Original 
# Doc99       Query       Doc47       Doc97       Doc58       Doc36       Doc91       Doc52       Doc14 
# 1.000000000 1.000000000 0.011416733 0.006815662 0.006313162 0.005684204 0.005347815 0.005084784 0.004979293 
#       Doc2 
# 0.004323966 
# First Variant
#      Doc99       Query       Doc47       Doc97       Doc58       Doc36       Doc91       Doc52       Doc14 
# 1.000000000 1.000000000 0.011416733 0.006815662 0.006313162 0.005684204 0.005347815 0.005084784 0.004979293 
#       Doc2 
# 0.004323966 
# Second Variant
#  Doc99     Query     Doc47      Doc3     Doc97     Doc75     Doc91     Doc58     Doc36      Doc2 
# 1.0000000 1.0000000 0.2074686 0.1391149 0.1377114 0.1267981 0.1263304 0.1166328 0.1159699 0.1138631 
# Third Variant
#  Doc99     Query     Doc47      Doc2      Doc3     Doc91     Doc86      Doc1     Doc97     Doc95 
# 1.0000000 1.0000000 0.2919373 0.2365305 0.2144333 0.2099129 0.2096305 0.1827213 0.1363900 0.1273157 

# Comparison
# Variant 1 
# Provides baseline. Most commonly used version of TF-IDF.
# Variant 2
# By adding log to the number of occurances this allows smaller documents to carry more weight making them closer in weight to larger 
# documents, thus allowing rarer words to be more prevlant. While adding 1 to both TF and IDF makes values inherently worth 
# more, skewing the results.This is shown by the cosine similarity values being almost double those of the other variants. 
# Variant 3 
# In this variant term frequency is equal to the number of occurances, meaning larger documents will carry more weight. This can be
# seen by Doc 2 appearing in the results after not being seen in any similarity test in the previous results. 
